{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-01T15:05:49.525764Z",
     "iopub.status.busy": "2022-01-01T15:05:49.525407Z",
     "iopub.status.idle": "2022-01-01T15:05:49.587994Z",
     "shell.execute_reply": "2022-01-01T15:05:49.587037Z",
     "shell.execute_reply.started": "2022-01-01T15:05:49.525688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión pandas: 1.0.5\n",
      "Versión numpy: 1.19.5\n",
      "Versión matplotlib: 3.2.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pmdarima.arima import ndiffs, nsdiffs\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from scipy.stats import uniform\n",
    "import seaborn as sns\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from metricas import calculo_metricas\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Mostramos las versiones de los módulos para posibles reproducciones del código\n",
    "\n",
    "print('Versión pandas:', pd.__version__)\n",
    "print('Versión numpy:', np.__version__)\n",
    "print('Versión matplotlib:', matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y preparación de la base de datos convirtiéndola en un np.array\n",
    "\n",
    "Aquí están todas las variables que podemos considerar, para no ir tratando con varios dataframes, se junta en uno todas las posibles variables y posteriormente se selecciona un conjunto de variables, así para ponerlo en los resultados podemos escoger directamente el conjunto de variables utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T15:05:49.589706Z",
     "iopub.status.busy": "2022-01-01T15:05:49.589352Z",
     "iopub.status.idle": "2022-01-01T15:05:49.977146Z",
     "shell.execute_reply": "2022-01-01T15:05:49.976363Z",
     "shell.execute_reply.started": "2022-01-01T15:05:49.58967Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/dataframe.csv', index_col = 0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.drop(columns = [\"Festivo_Regional\", \"Humedad_Relativa\", \"Precipitacion\", \"Radiacion\", \"Velocidad_Viento\"])\n",
    "\n",
    "df.loc[:, \"lag_24\"] = df.Spot_electricidad.shift(24)\n",
    "df.loc[:, \"lag_48\"] = df.Spot_electricidad.shift(48)\n",
    "df.loc[:, \"lag_1_semana\"] = df.Spot_electricidad.shift(24*7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distinto conjunto de variables**\n",
    "\n",
    "Importante que la variable Price esté definida como la tercera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T11:45:39.547227Z",
     "iopub.status.busy": "2021-07-16T11:45:39.546903Z",
     "iopub.status.idle": "2021-07-16T11:45:39.558003Z",
     "shell.execute_reply": "2021-07-16T11:45:39.557114Z",
     "shell.execute_reply.started": "2021-07-16T11:45:39.547196Z"
    }
   },
   "outputs": [],
   "source": [
    "variables = variables8\n",
    "\n",
    "raw_data = df[variables].values\n",
    "\n",
    "print(\"Raw_data: Dimensiones:\",np.shape(raw_data),\"Tipo de dato\", type(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T11:45:41.668231Z",
     "iopub.status.busy": "2021-07-16T11:45:41.667913Z",
     "iopub.status.idle": "2021-07-16T11:45:41.679308Z",
     "shell.execute_reply": "2021-07-16T11:45:41.678252Z",
     "shell.execute_reply.started": "2021-07-16T11:45:41.668202Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = raw_data.mean(axis=0)\n",
    "raw_data -= mean\n",
    "std = raw_data.std(axis=0)\n",
    "raw_data /= std\n",
    "\n",
    "print(\"Media por columna:\\n\",mean)\n",
    "print(\"Desviación estándar por columna:\\n\",std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del generador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T11:45:50.747461Z",
     "iopub.status.busy": "2021-07-16T11:45:50.747123Z",
     "iopub.status.idle": "2021-07-16T11:45:50.755939Z",
     "shell.execute_reply": "2021-07-16T11:45:50.754859Z",
     "shell.execute_reply.started": "2021-07-16T11:45:50.747414Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size =24, step = 1):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i+batch_size, max_index))\n",
    "            i += len(rows)\n",
    "            \n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j , row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][2]  # Aquí le pongo el 2 porque es la segunda columna el precio en df\n",
    "                                                    # y por tanto también lo será en el df.values\n",
    "            \n",
    "        yield samples, targets\n",
    "        \n",
    "lookback = 960\n",
    "step = 1\n",
    "delay = 24\n",
    "batch_size = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T11:45:55.194888Z",
     "iopub.status.busy": "2021-07-16T11:45:55.19456Z",
     "iopub.status.idle": "2021-07-16T11:45:55.204021Z",
     "shell.execute_reply": "2021-07-16T11:45:55.202844Z",
     "shell.execute_reply.started": "2021-07-16T11:45:55.19486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Así comienza el 1 de julio el test o validación. Si quiero poner algo como dias de test o algo así, no olvidar luego\n",
    "# ponerle al índice +1 porque si no, no coge el último dato\n",
    "\n",
    "indice = 10775\n",
    "\n",
    "train_min_index = 0\n",
    "train_max_index = 8568\n",
    "\n",
    "val_min_index = 12887 - lookback\n",
    "val_max_index = 17495\n",
    "\n",
    "test_min_index = 9048 - lookback\n",
    "test_max_index = 9073\n",
    "\n",
    "train_gen = generator(raw_data,\n",
    "                      lookback = lookback,\n",
    "                      delay = delay,\n",
    "                     min_index = train_min_index,\n",
    "                     max_index = train_max_index,\n",
    "                     shuffle = True,\n",
    "                     step = step,\n",
    "                     batch_size = batch_size)\n",
    "# Este por ahora no le vamos a hacer caso\n",
    "val_gen = generator(raw_data, lookback = lookback, delay = delay,min_index = val_min_index,max_index = val_max_index,step = step,batch_size = batch_size)\n",
    "\n",
    "test_gen = generator(raw_data,\n",
    "                      lookback = lookback,\n",
    "                      delay = delay,\n",
    "                     min_index = test_min_index,\n",
    "                     max_index = test_max_index,\n",
    "                     step = step,\n",
    "                     batch_size = batch_size)\n",
    "train_steps = int((train_max_index - train_min_index - lookback)/24)\n",
    "val_steps = int((val_max_index - val_min_index - lookback)/24)\n",
    "test_steps = int((test_max_index - test_min_index - lookback)/24)\n",
    "\n",
    "print(\"Steps de training\", train_steps)\n",
    "print(\"Steps de validación\",val_steps)\n",
    "print(\"Steps de test\", test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instancias de los generadores de entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T11:48:39.576916Z",
     "iopub.status.busy": "2021-07-16T11:48:39.576599Z",
     "iopub.status.idle": "2021-07-16T11:48:39.644473Z",
     "shell.execute_reply": "2021-07-16T11:48:39.642555Z",
     "shell.execute_reply.started": "2021-07-16T11:48:39.576886Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim= (lookback, raw_data.shape[-1]), activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(optimizer = 'adam', loss = 'mae')\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CON TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-08T08:56:19.393444Z",
     "iopub.status.idle": "2021-07-08T08:56:19.394367Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-08T08:56:19.396098Z",
     "iopub.status.idle": "2021-07-08T08:56:19.396875Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "with tpu_strategy.scope():\n",
    "    model = tf.keras.Sequential() # define your model normally\n",
    "    model.add(tf.keras.layers.LSTM(64,\n",
    "                    dropout = 0,\n",
    "                    return_sequences = True,\n",
    "                    input_shape = (lookback, raw_data.shape[-1])))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(32,\n",
    "                     dropout = 0))\n",
    "    model.add(tf.keras.layers.Dense(50))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mae')\n",
    "    \n",
    "model.fit(train_gen,steps_per_epoch = 100, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T08:05:18.395747Z",
     "iopub.status.busy": "2021-07-16T08:05:18.395424Z",
     "iopub.status.idle": "2021-07-16T08:18:27.182961Z",
     "shell.execute_reply": "2021-07-16T08:18:27.182194Z",
     "shell.execute_reply.started": "2021-07-16T08:05:18.395718Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis= -1))\n",
    "    \n",
    "def mape(y_true, y_pred):\n",
    "    y_real = y_true* std[2] + mean[2]\n",
    "    y_p = y_pred * std[2] + mean[2]\n",
    "    return K.abs((K.mean(y_p - y_real, axis = -1))/ y_real) * 100\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_mape', patience=200, restore_best_weights=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(64,\n",
    "                    dropout = 0.2,\n",
    "                    return_sequences = True,\n",
    "                    input_shape = (lookback, raw_data.shape[-1])))\n",
    "\n",
    "# model.add(layers.LSTM(64,\n",
    "#                       return_sequences = True,\n",
    "#                       dropout = 0.2))\n",
    "\n",
    "model.add(layers.LSTM(32,\n",
    "                     dropout = 0.2))\n",
    "\n",
    "model.add(layers.Dense(50))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mae')\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T08:20:17.817503Z",
     "iopub.status.busy": "2021-07-16T08:20:17.816991Z",
     "iopub.status.idle": "2021-07-16T08:20:17.87627Z",
     "shell.execute_reply": "2021-07-16T08:20:17.875415Z",
     "shell.execute_reply.started": "2021-07-16T08:20:17.81745Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T09:27:44.570556Z",
     "iopub.status.busy": "2021-07-15T09:27:44.570229Z",
     "iopub.status.idle": "2021-07-15T09:27:45.254186Z",
     "shell.execute_reply": "2021-07-15T09:27:45.25334Z",
     "shell.execute_reply.started": "2021-07-15T09:27:44.570525Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.LSTM(128,\n",
    "                    dropout = 0.2,\n",
    "                    return_sequences = True,\n",
    "                    input_shape = (lookback, raw_data.shape[-1])))\n",
    "\n",
    "model.add(layers.LSTM(64,\n",
    "                      return_sequences = True,\n",
    "                      dropout = 0.2))\n",
    "\n",
    "model.add(layers.LSTM(32,\n",
    "                     dropout = 0.2))\n",
    "\n",
    "model.add(layers.Dense(50))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mae')\n",
    "model.set_weights(weights)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo guardamos por si vamos a realizar varias pruebas, así empiezan todas del mismo punto de partida**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-16T08:43:12.703032Z",
     "iopub.status.busy": "2021-07-16T08:43:12.702631Z",
     "iopub.status.idle": "2021-07-16T09:04:59.167493Z",
     "shell.execute_reply": "2021-07-16T09:04:59.166653Z",
     "shell.execute_reply.started": "2021-07-16T08:43:12.703002Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Todas estas pruebas se hacen para un modelo previamente entrenado que en cada iteración se carga\n",
    "# con el model = keras.load_models('nombre_modelo.h5')\n",
    "\n",
    "# Épocas de reentrenamiento\n",
    "epochs = [1,5,16,20]\n",
    "\n",
    "for j in epochs:\n",
    "#     model = load_model('model.h5',custom_objects = {'root_mean_squared_error':root_mean_squared_error(y_true, y_pred),\n",
    "#                                                'mape':mape(y_true,y_pred)})\n",
    "    o,p, objetivo, prediccion, final_objetivo, final_prediccion = [], [], [], [], [], []\n",
    "    #model = load_model('/kaggle/input/modelo-intento-15-julio/model (2).h5')\n",
    "    model = load_model('model.h5')\n",
    "    final_prediccion = []\n",
    "    final_objetivo = []    \n",
    "    \n",
    "    # De esta manera está entrenado para reentrenar cada 24 horas\n",
    "    for i in range(0,120):\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        indice = 8568\n",
    "        # indice 12887\n",
    "        #indice = 10775\n",
    "        train_min_index = indice - lookback + (i-1)*24\n",
    "        train_max_index = indice + i*24\n",
    "        test_min_index = indice - lookback + i*24\n",
    "        test_max_index = indice + (i+1)*24\n",
    "        \n",
    "        train_steps = int((train_max_index - train_min_index - lookback)/24)\n",
    "    \n",
    "        test_steps = int((test_max_index - test_min_index - lookback)/24)\n",
    "        \n",
    "        train_gen = generator(raw_data,lookback = lookback,delay = delay,min_index = train_min_index,max_index = train_max_index,\n",
    "                              shuffle = False,step = step,batch_size = batch_size)\n",
    "        \n",
    "        test_gen = generator(raw_data,lookback = lookback,delay = delay,min_index = test_min_index,max_index = test_max_index,\n",
    "                             step = step,batch_size = batch_size)\n",
    "        #Reentrenamiento\n",
    "        if i == 0:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            model.fit(train_gen, steps_per_epoch = 4, epochs = j, verbose = 0)\n",
    "        \n",
    "        # Conversión\n",
    "        semana_data = []\n",
    "        semana_target = []\n",
    "        for i in range(test_steps):\n",
    "            data, target = next(test_gen)\n",
    "            semana_data.append(data)\n",
    "            semana_target.append(target)\n",
    "            \n",
    "        semana_data = np.array(semana_data)\n",
    "        semana_target = np.array(semana_target)\n",
    "        preds = []\n",
    "        for rows in semana_data:\n",
    "            preds.append(model.predict(rows))\n",
    "        preds = np.array(preds).flatten()\n",
    "        \n",
    "        objetivo = (semana_target * std[2] + mean[2]).flatten()\n",
    "        prediccion = preds * std[2] + mean[2]\n",
    "\n",
    "        final_prediccion.append(prediccion)\n",
    "        final_objetivo.append(objetivo)\n",
    "        \n",
    "    print(\"Para\", j,\"epocas de reentrenamiento y 1 dia de reentrenamiento resulta:\")\n",
    "    o = np.array(final_objetivo).flatten()\n",
    "    p = np.array(final_prediccion).flatten()\n",
    "    print(\"MAPE global:\",(abs(o - p)/o).mean() *100)\n",
    "    data = pd.DataFrame({\"Real\":pd.Series(o), \"Pred\":pd.Series(p)})\n",
    "    ruta = \"backtesting_\" + str(j) + \".csv\"\n",
    "    #data.to_csv(ruta)\n",
    "    import pylab\n",
    "    params = {\"legend.fontsize\":12,\n",
    "             \"figure.figsize\":(12,8),\n",
    "             \"axes.labelsize\":15,\n",
    "             \"axes.titlesize\":15,\n",
    "             \"xtick.labelsize\":15,\n",
    "             \"ytick.labelsize\":15}\n",
    "    pylab.rcParams.update(params)\n",
    "    \n",
    "    plt.plot(range(len(data)),data.Real)\n",
    "    plt.plot(range(len(data)), data.Pred)\n",
    "    plt.title(\"Prediccion de la red LSTM\")\n",
    "    plt.show()\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    def wmape(actual, forecast):\n",
    "        se_mape = abs(actual-forecast)/actual\n",
    "        ft_actual_sum = actual.sum()\n",
    "        se_actual_prod_mape = actual * se_mape\n",
    "        ft_actual_prod_mape_sum = se_actual_prod_mape.sum()\n",
    "        ft_wmape_forecast = ft_actual_prod_mape_sum / ft_actual_sum\n",
    "        return ft_wmape_forecast\n",
    "    \n",
    "    \n",
    "    def atipico(data, p):\n",
    "        lon_ini = len(data)\n",
    "        db = data[data.loc[:,'Real'] >= p]\n",
    "        lon_fin = len(db)\n",
    "        \n",
    "        mape_mediano_orig = round((abs((data.Pred - data.Real ))/ data.Real).median()*100, 3)\n",
    "        mape_orig = round((abs((data.Pred - data.Real ))/ data.Real).mean()*100, 3)\n",
    "        mae_mean = round((abs(data.Pred - data.Real )).mean(), 3)\n",
    "        mae_median = round((abs(data.Pred - data.Real )).median(), 3)\n",
    "        mse = mean_squared_error(data.Real, data.Pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmse_orig = round(rmse, 3)\n",
    "        wmape_orig = wmape(data.Real, data.Pred) * 100\n",
    "        \n",
    "        mape_mediano_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).median()*100, 3)\n",
    "        mape_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).mean()*100, 3)\n",
    "        mae_mean = round((abs(db.Pred - db.Real )).mean(), 3)\n",
    "        mae_median = round((abs(db.Pred - db.Real )).median(), 3)\n",
    "        mse = mean_squared_error(db.Real, db.Pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        rmse_nuevo = round(rmse, 3)\n",
    "        wmape_nuevo = wmape(db.Real, db.Pred) * 100\n",
    "        \n",
    "        r = pd.DataFrame({\"Original\":[mape_orig, mape_mediano_orig, mae_mean,mae_median, rmse_orig, wmape_orig],\n",
    "                         \"Sin_atípicos\":[mape_nuevo, mape_mediano_nuevo, mae_mean,mae_median, rmse_nuevo, wmape_nuevo]},\n",
    "                         index = [\"MAPE\", \"MAPE_mediano\",\"MAE_media\", \"MAE_mediano\",\"RMSE\",\"WMAPE\"])\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    atip = (atipico(data,4.5))\n",
    "    print(atip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:57:52.342196Z",
     "iopub.status.busy": "2021-07-15T12:57:52.341858Z",
     "iopub.status.idle": "2021-07-15T12:57:52.718916Z",
     "shell.execute_reply": "2021-07-15T12:57:52.717581Z",
     "shell.execute_reply.started": "2021-07-15T12:57:52.342167Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Para\", j,\"epocas de reentrenamiento y 1 dia de reentrenamiento resulta:\")\n",
    "o = np.array(final_objetivo).flatten()\n",
    "p = np.array(final_prediccion).flatten()\n",
    "print(\"MAPE global:\",(abs(o - p)/o).mean() *100)\n",
    "data = pd.DataFrame({\"Real\":pd.Series(o), \"Pred\":pd.Series(p)})\n",
    "ruta = \"backtesting_\" + str(j) + \".csv\"\n",
    "#data.to_csv(ruta)\n",
    "import pylab\n",
    "params = {\"legend.fontsize\":12,\n",
    "         \"figure.figsize\":(12,8),\n",
    "         \"axes.labelsize\":15,\n",
    "         \"axes.titlesize\":15,\n",
    "         \"xtick.labelsize\":15,\n",
    "         \"ytick.labelsize\":15}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "plt.plot(range(len(data)),data.Real)\n",
    "plt.plot(range(len(data)), data.Pred)\n",
    "plt.title(\"Prediccion de la red LSTM\")\n",
    "plt.show()\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def wmape(actual, forecast):\n",
    "    se_mape = abs(actual-forecast)/actual\n",
    "    ft_actual_sum = actual.sum()\n",
    "    se_actual_prod_mape = actual * se_mape\n",
    "    ft_actual_prod_mape_sum = se_actual_prod_mape.sum()\n",
    "    ft_wmape_forecast = ft_actual_prod_mape_sum / ft_actual_sum\n",
    "    return ft_wmape_forecast\n",
    "\n",
    "\n",
    "def atipico(data, p):\n",
    "    lon_ini = len(data)\n",
    "    db = data[data.loc[:,'Real'] >= p]\n",
    "    lon_fin = len(db)\n",
    "    \n",
    "    mape_mediano_orig = round((abs((data.Pred - data.Real ))/ data.Real).median()*100, 3)\n",
    "    mape_orig = round((abs((data.Pred - data.Real ))/ data.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(data.Pred - data.Real )).mean(), 3)\n",
    "    mae_median = round((abs(data.Pred - data.Real )).median(), 3)\n",
    "    mse = mean_squared_error(data.Real, data.Pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_orig = round(rmse, 3)\n",
    "    wmape_orig = wmape(data.Real, data.Pred) * 100\n",
    "    \n",
    "    mape_mediano_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).median()*100, 3)\n",
    "    mape_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(db.Pred - db.Real )).mean(), 3)\n",
    "    mae_median = round((abs(db.Pred - db.Real )).median(), 3)\n",
    "    mse = mean_squared_error(db.Real, db.Pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_nuevo = round(rmse, 3)\n",
    "    wmape_nuevo = wmape(db.Real, db.Pred) * 100\n",
    "    \n",
    "    r = pd.DataFrame({\"Original\":[mape_orig, mape_mediano_orig, mae_mean,mae_median, rmse_orig, wmape_orig],\n",
    "                     \"Sin_atípicos\":[mape_nuevo, mape_mediano_nuevo, mae_mean,mae_median, rmse_nuevo, wmape_nuevo]},\n",
    "                     index = [\"MAPE\", \"MAPE_mediano\",\"MAE_media\", \"MAE_mediano\",\"RMSE\",\"WMAPE\"])\n",
    "    \n",
    "    return r\n",
    "\n",
    "atip = (atipico(data,4.5))\n",
    "print(atip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:58:13.9636Z",
     "iopub.status.busy": "2021-07-15T12:58:13.963197Z",
     "iopub.status.idle": "2021-07-15T12:58:13.969983Z",
     "shell.execute_reply": "2021-07-15T12:58:13.968825Z",
     "shell.execute_reply.started": "2021-07-15T12:58:13.963567Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T12:45:24.958295Z",
     "iopub.status.busy": "2021-07-12T12:45:24.957978Z",
     "iopub.status.idle": "2021-07-12T12:45:25.139073Z",
     "shell.execute_reply": "2021-07-12T12:45:25.136044Z",
     "shell.execute_reply.started": "2021-07-12T12:45:24.958266Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"lag\"] = data[\"Pred\"].shift(-24)\n",
    "r = data.dropna()\n",
    "r[[\"Real\",\"lag\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-08T08:56:19.418318Z",
     "iopub.status.idle": "2021-07-08T08:56:19.419179Z"
    }
   },
   "outputs": [],
   "source": [
    "mapes = [15.185, 14.168, 12.869, 12.304, 12.147, 11.628, 11.685, 12.35]\n",
    "plt.plot([1,2,4,5,8, 10, 12, 15], mapes)\n",
    "plt.ylabel('MAPE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Evolución del MAPE con el número de épocas y 4 pasos por época.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T12:47:58.779849Z",
     "iopub.status.busy": "2021-07-12T12:47:58.779507Z",
     "iopub.status.idle": "2021-07-12T12:47:58.811307Z",
     "shell.execute_reply": "2021-07-12T12:47:58.810516Z",
     "shell.execute_reply.started": "2021-07-12T12:47:58.77982Z"
    }
   },
   "outputs": [],
   "source": [
    "def atipico(data, p):\n",
    "    lon_ini = len(data)\n",
    "    db = data[data.loc[:,'Real'] >= p]\n",
    "    lon_fin = len(db)\n",
    "    \n",
    "    mape_mediano_orig = round((abs((data.Pred - data.Real ))/ data.Real).median()*100, 3)\n",
    "    mape_orig = round((abs((data.Pred - data.Real ))/ data.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(data.Pred - data.Real )).mean(), 3)\n",
    "    mae_median = round((abs(data.Pred - data.Real )).median(), 3)\n",
    "    mse = mean_squared_error(data.Real, data.Pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_orig = round(rmse, 3)\n",
    "    wmape_orig = wmape(data.Real, data.Pred) * 100\n",
    "    \n",
    "    mape_mediano_nuevo = round((abs((db.lag - db.Real ))/ db.Real).median()*100, 3)\n",
    "    mape_nuevo = round((abs((db.lag - db.Real ))/ db.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(db.lag - db.Real )).mean(), 3)\n",
    "    mae_median = round((abs(db.lag - db.Real )).median(), 3)\n",
    "    mse = mean_squared_error(db.Real, db.lag)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_nuevo = round(rmse, 3)\n",
    "    wmape_nuevo = wmape(db.Real, db.Pred) * 100\n",
    "    \n",
    "    r = pd.DataFrame({\"Original\":[mape_orig, mape_mediano_orig, mae_mean,mae_median, rmse_orig, wmape_orig],\n",
    "                     \"Sin_atípicos\":[mape_nuevo, mape_mediano_nuevo, mae_mean,mae_median, rmse_nuevo, wmape_nuevo]},\n",
    "                     index = [\"MAPE\", \"MAPE_mediano\",\"MAE_media\", \"MAE_mediano\",\"RMSE\",\"WMAPE\"])\n",
    "    \n",
    "    return r\n",
    "atipico(r, 4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código para ejecutar si solo se realiza una prueba sin Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-08T08:56:19.422913Z",
     "iopub.status.idle": "2021-07-08T08:56:19.423803Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model.h5')\n",
    "final_prediccion = []\n",
    "final_objetivo = []\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(0,280):\n",
    "    train_min_index = 9048-lookback + (i-1)*24\n",
    "    train_max_index = 9048 + i*24\n",
    "    test_min_index = 8088 + i*24\n",
    "    test_max_index = 9073 + i*24\n",
    "    \n",
    "    train_steps = int((train_max_index - train_min_index - lookback)/24)\n",
    "    test_steps = int((test_max_index - test_min_index - lookback)/24)\n",
    "    \n",
    "    train_gen = generator(raw_data,\n",
    "                      lookback = lookback,\n",
    "                      delay = delay,\n",
    "                     min_index = train_min_index,\n",
    "                     max_index = train_max_index,\n",
    "                     shuffle = False,\n",
    "                     step = step,\n",
    "                     batch_size = batch_size)\n",
    "    \n",
    "    test_gen = generator(raw_data,\n",
    "                      lookback = lookback,\n",
    "                      delay = delay,\n",
    "                     min_index = test_min_index,\n",
    "                     max_index = test_max_index,\n",
    "                     step = step,\n",
    "                     batch_size = batch_size)\n",
    "    #Reentrenamiento\n",
    "    if i ==0:\n",
    "        pass\n",
    "    else:\n",
    "        model.fit(train_gen, steps_per_epoch = 24, epochs = 2)\n",
    "    \n",
    "    # Conversión\n",
    "    semana_data = []\n",
    "    semana_target = []\n",
    "    for i in range(test_steps):\n",
    "        data, target = next(test_gen)\n",
    "        semana_data.append(data)\n",
    "        semana_target.append(target)\n",
    "        \n",
    "    semana_data = np.array(semana_data)\n",
    "    semana_target = np.array(semana_target)\n",
    "    \n",
    "    preds = []\n",
    "    for rows in semana_data:\n",
    "        preds.append(model.predict(rows))\n",
    "    preds = np.array(preds).flatten()\n",
    "    \n",
    "    objetivo = (semana_target * std[2] + mean[2]).flatten()\n",
    "    prediccion = preds * std[2] + mean[2]\n",
    "\n",
    "    final_prediccion.append(prediccion)\n",
    "    final_objetivo.append(objetivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-08T08:56:19.425557Z",
     "iopub.status.idle": "2021-07-08T08:56:19.426387Z"
    }
   },
   "outputs": [],
   "source": [
    "o = np.array(final_objetivo).flatten()\n",
    "p = np.array(final_prediccion).flatten()\n",
    "print(\"MAPE:\",(abs(o - p)/o).mean() *100)\n",
    "data = pd.DataFrame({\"Real\":pd.Series(o), \"Pred\":pd.Series(p)})\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def wmape(actual, forecast):\n",
    "    # we take two series and calculate an output a wmape from it\n",
    "\n",
    "    # make a series called mape\n",
    "    se_mape = abs(actual-forecast)/actual\n",
    "\n",
    "    # get a float of the sum of the actual\n",
    "    ft_actual_sum = actual.sum()\n",
    "\n",
    "    # get a series of the multiple of the actual & the mape\n",
    "    se_actual_prod_mape = actual * se_mape\n",
    "\n",
    "    # summate the prod of the actual and the mape\n",
    "    ft_actual_prod_mape_sum = se_actual_prod_mape.sum()\n",
    "\n",
    "    # float: wmape of forecast\n",
    "    ft_wmape_forecast = ft_actual_prod_mape_sum / ft_actual_sum\n",
    "\n",
    "    # return a float\n",
    "    return ft_wmape_forecast\n",
    "\n",
    "\n",
    "def atipico(data, p):\n",
    "    lon_ini = len(data)\n",
    "    db = data[data.loc[:,'Real'] >= p]\n",
    "    lon_fin = len(db)\n",
    "    #print(\"Se han eliminado\", lon_ini - lon_fin, \"datos\")\n",
    "    #print(\"Se han eliminado el \", ((lon_ini - lon_fin) / lon_ini)*100, \"%\")\n",
    "    \n",
    "    mape_mediano_orig = round((abs((data.Pred - data.Real ))/ data.Real).median()*100, 3)\n",
    "    mape_orig = round((abs((data.Pred - data.Real ))/ data.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(data.Pred - data.Real )).mean(), 3)\n",
    "    mae_median = round((abs(data.Pred - data.Real )).median(), 3)\n",
    "    mse = mean_squared_error(data.Real, data.Pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_orig = round(rmse, 3)\n",
    "    wmape_orig = wmape(data.Real, data.Pred) * 100\n",
    "    \n",
    "    mape_mediano_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).median()*100, 3)\n",
    "    mape_nuevo = round((abs((db.Pred - db.Real ))/ db.Real).mean()*100, 3)\n",
    "    mae_mean = round((abs(db.Pred - db.Real )).mean(), 3)\n",
    "    mae_median = round((abs(db.Pred - db.Real )).median(), 3)\n",
    "    mse = mean_squared_error(db.Real, db.Pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse_nuevo = round(rmse, 3)\n",
    "    wmape_nuevo = wmape(db.Real, db.Pred) * 100\n",
    "    \n",
    "    r = pd.DataFrame({\"Original\":[mape_orig, mape_mediano_orig, mae_mean,mae_median, rmse_orig, wmape_orig],\n",
    "                     \"Sin_atípicos\":[mape_nuevo, mape_mediano_nuevo, mae_mean,mae_median, rmse_nuevo, wmape_nuevo]},\n",
    "                     index = [\"MAPE\", \"MAPE_mediano\",\"MAE_media\", \"MAE_mediano\",\"RMSE\",\"WMAPE\"])\n",
    "    \n",
    "    return r\n",
    "\n",
    "atipico(data,4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación del modelo del paper híbrido CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-08T09:42:13.885198Z",
     "iopub.status.busy": "2021-07-08T09:42:13.884821Z",
     "iopub.status.idle": "2021-07-08T09:44:03.18045Z",
     "shell.execute_reply": "2021-07-08T09:44:03.179151Z",
     "shell.execute_reply.started": "2021-07-08T09:42:13.885164Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# with tpu_strategy.scope():\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3, input_shape=(lookback,raw_data.shape[-1])))\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3, input_shape=(lookback,raw_data.shape[-1])))\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=3))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.RepeatVector(24))\n",
    "\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.LSTM(16))\n",
    "model.add(layers.Dense(100))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mae', metrics=[root_mean_squared_error, mape])\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 200,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = val_steps,\n",
    "                    callbacks = [early_stop])\n",
    "\n",
    "# model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
