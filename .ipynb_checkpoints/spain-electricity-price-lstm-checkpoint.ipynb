{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of the require modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:35.567784Z",
     "iopub.status.busy": "2022-03-13T17:33:35.566999Z",
     "iopub.status.idle": "2022-03-13T17:33:40.798952Z",
     "shell.execute_reply": "2022-03-13T17:33:40.798190Z",
     "shell.execute_reply.started": "2022-03-13T17:33:35.567740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data preprocessing and visualization\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Deep learning modules\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# from metricas import calculo_metricas\n",
    "import gc\n",
    "\n",
    "# Versions of some of the packages\n",
    "print('Version pandas:', pd.__version__)\n",
    "print('Version numpy:', np.__version__)\n",
    "print('Version seaborn', sns.__version__)\n",
    "print('Version tensorflow: ', tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:40.801247Z",
     "iopub.status.busy": "2022-03-13T17:33:40.800649Z",
     "iopub.status.idle": "2022-03-13T17:33:40.817812Z",
     "shell.execute_reply": "2022-03-13T17:33:40.817164Z",
     "shell.execute_reply.started": "2022-03-13T17:33:40.801207Z"
    }
   },
   "outputs": [],
   "source": [
    "def mae(predictions, targets):\n",
    "    \"\"\"Función que calcula el MAE (Mean Absolute Error) entre la predicción\n",
    "    y la variable real.\n",
    "    Params:\n",
    "        - predictions: array de las predicciones\n",
    "        - targets: array de los valores reales.\n",
    "    \"\"\"\n",
    "    return abs(predictions - targets)\n",
    "\n",
    "def mape(predictions, targets):\n",
    "    \"\"\"Funtion that computes the MAPE (Mean Absolute Percentage Error) between the prediction and the target variable\n",
    "    Params:\n",
    "        - predictions: array with the predictions\n",
    "        - targets: array with real values\n",
    "    \"\"\"\n",
    "    return abs((targets - predictions) / targets) * 100\n",
    "\n",
    "def wmape(predictions, targets):\n",
    "    \"\"\"Funtion that computes the WMAPE (Weight Mean Absolute Percentage Error) between the prediction and the\n",
    "    target variable.\n",
    "    Params:\n",
    "        - predictions: array with the predictions\n",
    "        - targets: array with real values\n",
    "    \"\"\"\n",
    "    return np.sum(abs(targets - predictions)) / np.sum(targets) * 100\n",
    "\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    \"\"\"Funtion that computes the RMSE (Root Mean Squared Error) between the prediction and the target variable\n",
    "    Params:\n",
    "        - predictions: array with the predictions\n",
    "        - targets: array with real values\n",
    "    \"\"\"\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def rmse_lstm(y_true, y_pred):\n",
    "    \"\"\"Monitorizing the rmse with denormalized values in the training of the LSTM model\n",
    "    Params:\n",
    "        - y_pred: array with the predictions\n",
    "        - y_true: array with real values\n",
    "            \"\"\"\n",
    "    y_real = y_true* std[0] + mean[0]\n",
    "    y_p = y_pred * std[0] + mean[0]\n",
    "    \n",
    "    return K.sqrt(K.mean(K.square(y_p - y_real), axis= -1))\n",
    "\n",
    "\n",
    "def wmape_lstm(predictions, targets):\n",
    "    \"\"\"Monitorizing the wmape with denormalized values in the training of the LSTM model\n",
    "    Params:\n",
    "        - predictions: array de las predicciones\n",
    "        - targets: array de los valores reales.\n",
    "    \"\"\"\n",
    "    y_real = targets* std[0] + mean[0]\n",
    "    y_p = predictions * std[0] + mean[0]\n",
    "    \n",
    "    return K.sum(K.abs(y_real - y_p)) / K.sum(y_real) * 100\n",
    "\n",
    "def calculo_metricas(dataframe):\n",
    "    \"\"\"Function that performs all the metrics that we want to compare.\n",
    "    Params:\n",
    "        - dataframe: a pd.Dataframe with two columns\n",
    "            + Pred: column with the predictions\n",
    "            + Real: column with real values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cálculos de las métricas de MAE y MAPE\n",
    "    dataframe.loc[:, \"MAE\"] = mae(dataframe.Pred, dataframe.Real)\n",
    "    dataframe.loc[:, \"MAPE\"] = mape(dataframe.Pred, dataframe.Real)\n",
    "    \n",
    "    # Dataframe de predicción de tendencia\n",
    "    up_down_price = dataframe[[\"Pred\", \"Real\"]].diff(1).dropna()\n",
    "    up_down_price.loc[:, \"Ind_Pred\"] = [1 if data > 0 else 0 for data in up_down_price.Pred]\n",
    "    up_down_price.loc[:, \"Ind_Real\"] = [1 if data > 0 else 0 for data in up_down_price.Real]\n",
    "    up_down_price.loc[:, \"Aciertos\"] = up_down_price.loc[:, \"Ind_Pred\"] == up_down_price.loc[:, \"Ind_Real\"]\n",
    "    \n",
    "    # Creación del dataframe que se mostrará por pantalla\n",
    "    metrica = pd.DataFrame({\"MAE\" : round(float(dataframe[['MAE']].mean()), 2),\n",
    "                            \"MAE (median)\" : round(float(np.median(dataframe[['MAE']])), 2),\n",
    "                            \"MAPE\" : round(float(dataframe[['MAPE']].mean()), 2),\n",
    "                            \"WMAPE\" : round(wmape(dataframe.Pred, dataframe.Real), 2),\n",
    "                            \"RMSE\" : round(rmse(dataframe.Pred, dataframe.Real), 2),\n",
    "                            \"% Trend\" : round(up_down_price.Aciertos.value_counts()[1] / \n",
    "                                              up_down_price.Aciertos.value_counts().sum() * 100, 2)}, index = [0])\n",
    "    \n",
    "    # Mostramos por pantalla el dataframe de métricas\n",
    "    print(metrica)                 \n",
    "                            \n",
    "    # Dataframe de métricas por meses                        \n",
    "    return dataframe, metrica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and preparation of the database\n",
    "\n",
    "We read the dataframe already preprocessed, delete the variables that we have already analyzed that are not important and create the lags of 24 and 48 hours and 1 week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:46.006313Z",
     "iopub.status.busy": "2022-03-13T17:33:46.005461Z",
     "iopub.status.idle": "2022-03-13T17:33:46.592617Z",
     "shell.execute_reply": "2022-03-13T17:33:46.591945Z",
     "shell.execute_reply.started": "2022-03-13T17:33:46.006259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe reading\n",
    "df = pd.read_csv('/kaggle/input/spain-electricity-price/dataframe.csv', index_col = 0)\n",
    "\n",
    "# Index conversion to datetime\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Drop useless columns\n",
    "df = df.drop(columns = [\"Festivo_Regional\", \"Humedad_Relativa\", \"Precipitacion\", \"Radiacion\", \"Velocidad_Viento\"])\n",
    "\n",
    "# Create lags columns\n",
    "df.loc[:, \"lag_24\"] = df.Spot_electricidad.shift(24)\n",
    "df.loc[:, \"lag_48\"] = df.Spot_electricidad.shift(48)\n",
    "df.loc[:, \"lag_1_semana\"] = df.Spot_electricidad.shift(24*7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief exploration of the dataframe and the target variable\n",
    "\n",
    "48 variables (all numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:48.796470Z",
     "iopub.status.busy": "2022-03-13T17:33:48.796044Z",
     "iopub.status.idle": "2022-03-13T17:33:49.058436Z",
     "shell.execute_reply": "2022-03-13T17:33:49.057817Z",
     "shell.execute_reply.started": "2022-03-13T17:33:48.796435Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:52.205580Z",
     "iopub.status.busy": "2022-03-13T17:33:52.205040Z",
     "iopub.status.idle": "2022-03-13T17:33:52.515502Z",
     "shell.execute_reply": "2022-03-13T17:33:52.514757Z",
     "shell.execute_reply.started": "2022-03-13T17:33:52.205542Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "sns.boxplot(x = df.Spot_electricidad.index.year, y = df.Spot_electricidad)\n",
    "plt.ylabel('Electricity Price €/MWh')\n",
    "plt.xlabel('Year')\n",
    "plt.title('Evolution of the average price of electricity in Spain from 2014 to 2021')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We note the importance of trying to predict this variable since in the last year it has deviated a lot from the historical and try to explain this increase.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:33:55.457073Z",
     "iopub.status.busy": "2022-03-13T17:33:55.456649Z",
     "iopub.status.idle": "2022-03-13T17:34:00.084358Z",
     "shell.execute_reply": "2022-03-13T17:34:00.083179Z",
     "shell.execute_reply.started": "2022-03-13T17:33:55.457034Z"
    }
   },
   "outputs": [],
   "source": [
    "time_series = df.Spot_electricidad.copy()\n",
    "time_series = time_series.loc[:]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    x= time_series.index.tolist(),\n",
    "                    y=time_series,\n",
    "                    mode='lines',\n",
    "                    name='Real'))\n",
    "fig.update_layout(\n",
    "    title=\"Electricity hourly price evolution in Spain\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"€/MWh\",\n",
    "    legend_title=\"Spot price\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data normalization\n",
    "\n",
    "We will perform an hourly backtesting of the full year 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:34:04.483975Z",
     "iopub.status.busy": "2022-03-13T17:34:04.483262Z",
     "iopub.status.idle": "2022-03-13T17:34:04.511422Z",
     "shell.execute_reply": "2022-03-13T17:34:04.510618Z",
     "shell.execute_reply.started": "2022-03-13T17:34:04.483907Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop nulls, it's important.\n",
    "df = df.dropna()\n",
    "\n",
    "print('First date of the dataframe: ', df.index[0])\n",
    "print('Last date of the dataframe: ', df.index[-1])\n",
    "\n",
    "# Work with numpy arrays\n",
    "raw_data = df.values\n",
    "\n",
    "print(\"Dataframe dimensions\", np.shape(raw_data))\n",
    "\n",
    "# Standardization of the data\n",
    "mean = raw_data.mean(axis=0)\n",
    "raw_data -= mean\n",
    "std = raw_data.std(axis=0)\n",
    "raw_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:34:06.942461Z",
     "iopub.status.busy": "2022-03-13T17:34:06.941896Z",
     "iopub.status.idle": "2022-03-13T17:34:06.954131Z",
     "shell.execute_reply": "2022-03-13T17:34:06.953290Z",
     "shell.execute_reply.started": "2022-03-13T17:34:06.942424Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(data, order, target, lookback, min_index, max_index,\n",
    "              shuffle=False, delay = 24, batch_size =24, step = 1):\n",
    "    \"\"\"Function that creates the input data for the LSTM.\n",
    "    - Params:\n",
    "        - data: The original array of floating-point data\n",
    "        - order: order in the variables that have been used in the array\n",
    "        - lookback: How many timesteps back the input data should go.\n",
    "        - delay: How many timesteps in the future the target should be. We are going to predict the next 24 hours each day,\n",
    "                 so this value must be 24.\n",
    "        - min_index and max_index:Indices in the data array that delimit which timesteps to draw from.\n",
    "                                  This is useful for keeping a segment of the data for validation and another for testing.\n",
    "        - shuffle: Whether to shuffle the samples or draw them in chronological order.\n",
    "        - batch_size: The number of samples per batch. 24 four because of daily data\n",
    "\n",
    "    - Output:\n",
    "        - samples: regressor variables\n",
    "        - targets: targets variables\n",
    "        \"\"\"\n",
    "    target_index = order.index(target)\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i+batch_size, max_index))\n",
    "            i += len(rows)\n",
    "            \n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        \n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j , row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][target_index]  \n",
    "            \n",
    "        yield samples, targets\n",
    "        \n",
    "\n",
    "def get_index(df, date):\n",
    "    \"\"\"dñflahdf\n",
    "    Params:\n",
    "        - df: original dataframe without nan's\n",
    "        - date: str with the string in the format: \"%Y-%m-%d %H:%M%S\"\n",
    "    Output:\n",
    "        - Index: index of the dataframe which matches the date\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    for i, dt in enumerate(df.index):\n",
    "        if date == str(dt):\n",
    "            index = i\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:34:14.569013Z",
     "iopub.status.busy": "2022-03-13T17:34:14.568747Z",
     "iopub.status.idle": "2022-03-13T17:34:14.772221Z",
     "shell.execute_reply": "2022-03-13T17:34:14.771379Z",
     "shell.execute_reply.started": "2022-03-13T17:34:14.568985Z"
    }
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# ====================================================================================================\n",
    "lookback_days = 20\n",
    "lookback = 24 * lookback_days\n",
    "\n",
    "train_min_index = 0\n",
    "train_max_index = get_index(df.dropna(), '2021-01-01 00:00:00')\n",
    "\n",
    "# For the backtesting I'm going to ignore the validation data in the training because i'm going to make a backtesting\n",
    "# val_min_index = get_index(df.dropna(), '2021-06-01 00:00:00') - lookback\n",
    "# val_max_index = len(df.dropna())\n",
    "\n",
    "test_min_index = get_index(df.dropna(), '2021-01-01 00:00:00') - lookback\n",
    "test_max_index = len(df.dropna())\n",
    "\n",
    "print(train_min_index, train_max_index, test_min_index, test_max_index)\n",
    "train_gen = generator(data = raw_data,\n",
    "                     order = df.columns.tolist(),\n",
    "                     target = 'Spot_electricidad',\n",
    "                     lookback = lookback,\n",
    "                     min_index = train_min_index,\n",
    "                     max_index = train_max_index,\n",
    "                     shuffle = False) # It's True, but I'm going to try with False\n",
    "\n",
    "# val_gen = generator(data = raw_data,\n",
    "#                      order = df.columns.tolist(),\n",
    "#                      target = 'Spot_electricidad',\n",
    "#                      lookback = lookback,\n",
    "#                      min_index = val_min_index,\n",
    "#                      max_index = val_max_index)\n",
    "\n",
    "test_gen = generator(data = raw_data,\n",
    "                     order = df.columns.tolist(),\n",
    "                     target = 'Spot_electricidad',\n",
    "                     lookback = lookback,\n",
    "                     min_index = test_min_index,\n",
    "                     max_index = test_max_index)\n",
    "\n",
    "\n",
    "train_steps = int((train_max_index - train_min_index - lookback)/24)\n",
    "# val_steps = int((val_max_index - val_min_index - lookback)/24)\n",
    "test_steps = int((test_max_index - test_min_index - lookback)/24)\n",
    "\n",
    "print(\"Training steps\", train_steps)  # Two years (2019, 2020)\n",
    "# print(\"Steps de validación\",val_steps)\n",
    "print(\"Test steps\", test_steps) # Test data (backtesting, 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T16:00:41.343626Z",
     "iopub.status.busy": "2022-03-13T16:00:41.343369Z",
     "iopub.status.idle": "2022-03-13T16:19:11.267800Z",
     "shell.execute_reply": "2022-03-13T16:19:11.267046Z",
     "shell.execute_reply.started": "2022-03-13T16:00:41.343593Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(128,\n",
    "                    dropout = 0.1,\n",
    "                    return_sequences = True,\n",
    "                    input_shape = (lookback, raw_data.shape[-1])))\n",
    "\n",
    "model.add(layers.LSTM(64,\n",
    "                      return_sequences = True,\n",
    "                      dropout = 0.1))\n",
    "\n",
    "model.add(layers.LSTM(32,\n",
    "                      return_sequences = True,\n",
    "                      dropout = 0.1))\n",
    "\n",
    "model.add(layers.LSTM(16,\n",
    "                     dropout = 0.1))\n",
    "\n",
    "model.add(layers.Dense(50))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mae', metrics = [rmse_lstm, wmape_lstm])\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 150)\n",
    "\n",
    "# ============================================================================\n",
    "#If we want to see the validation data:\n",
    "\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_wmape',\n",
    "#                                             patience=50,\n",
    "#                                             restore_best_weights=True)\n",
    "\n",
    "# history = model.fit(train_gen,\n",
    "#                     steps_per_epoch = 100,\n",
    "#                     epochs = 200,\n",
    "#                     validation_data = val_gen,\n",
    "#                     validation_steps = val_steps,\n",
    "#                     callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-03-13T16:19:11.274075Z",
     "iopub.status.busy": "2022-03-13T16:19:11.269567Z",
     "iopub.status.idle": "2022-03-13T17:17:14.940593Z",
     "shell.execute_reply": "2022-03-13T17:17:14.939906Z",
     "shell.execute_reply.started": "2022-03-13T16:19:11.274037Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def backtesting_lstm(raw_data, df, target_variable, date, lookback,\n",
    "                     retraining_epochs, retraining_steps_per_epoch, backtesting_days):\n",
    "    \n",
    "    \"\"\"Function that performs the backtesting of the 2021 full year.\n",
    "    Params:\n",
    "        - raw_data: normalized data in np.array format\n",
    "        - df: original dataframe\n",
    "        - target_variable: name of the variable to be predicted\n",
    "        - date: this is the date which we want to start our backtesting\n",
    "        - retraining_epochs: number of epochs that we want to retrain every day\n",
    "        - retraining_steps_per_epoch: number of steps_per_epoch that we want to use for each retraining\n",
    "        - backtesting_days: number of days we want to perform our backtesting since the 'date' parameter\n",
    "    Output:\n",
    "        - dataframe: pd.Dataframe with two columns: Original value and predicted value with the dimensions \n",
    "                     (24 * backtesting_days, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    prediction, target = [], []\n",
    "    index = get_index(df.dropna(), date)\n",
    "    index_normalization = df.dropna().columns.tolist().index(target_variable)\n",
    "    \n",
    "    # El menos 1 para que no dé index error\n",
    "    for i in range(backtesting_days - 1):\n",
    "        gc.collect()\n",
    "        print(i)\n",
    "        train_min_index = index - lookback + (i-1)*24\n",
    "        train_max_index = index + i*24\n",
    "        test_min_index = index - lookback + i*24\n",
    "        test_max_index = index + (i+1)*24\n",
    "\n",
    "        test_steps = int((test_max_index - test_min_index - lookback)/24)\n",
    "\n",
    "        train_gen = generator(data = raw_data,\n",
    "                     order = df.columns.tolist(),\n",
    "                     target = 'Spot_electricidad',\n",
    "                     lookback = lookback,\n",
    "                     min_index = train_min_index,\n",
    "                     max_index = train_max_index,\n",
    "                     shuffle = False)\n",
    "        \n",
    "        test_gen = generator(data = raw_data,\n",
    "                     order = df.columns.tolist(),\n",
    "                     target = 'Spot_electricidad',\n",
    "                     lookback = lookback,\n",
    "                     min_index = test_min_index,\n",
    "                     max_index = test_max_index)\n",
    "        \n",
    "        # Retraining\n",
    "        if i == 0:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            model.fit(train_gen,\n",
    "                      steps_per_epoch = retraining_steps_per_epoch,\n",
    "                      epochs = retraining_epochs,\n",
    "                      verbose = 0)\n",
    "        \n",
    "        # Conversion of the test\n",
    "        generator_samples_test = []\n",
    "        generator_target_test = []\n",
    "        \n",
    "        for _ in range(test_steps):\n",
    "            data_sample, target_sample = next(test_gen)\n",
    "            generator_samples_test.append(data_sample)\n",
    "            generator_target_test.append(target_sample)\n",
    "            \n",
    "        generator_samples_test = np.array(generator_samples_test)\n",
    "        generator_target_test = np.array(generator_target_test)\n",
    "        \n",
    "        # Predictions\n",
    "        preds = []\n",
    "        for rows in generator_samples_test:\n",
    "            preds.append(model.predict(rows))\n",
    "        preds = np.array(preds).flatten()\n",
    "        \n",
    "        # Denormalization\n",
    "        target_aux = (generator_target_test * std[index_normalization] + mean[index_normalization]).flatten()\n",
    "        prediction_aux = preds * std[index_normalization] + mean[index_normalization]\n",
    "        \n",
    "        # =================================================== BORRAR ===================================================\n",
    "#         plt.plot(target_aux)\n",
    "#         plt.plot(prediction_aux)\n",
    "#         plt.show()\n",
    "        # ======================================================================================================\n",
    "        prediction.append(prediction_aux)\n",
    "        target.append(target_aux)\n",
    "        \n",
    "    # Flat the data and get the dataframe\n",
    "    final_target = np.array(target).flatten()\n",
    "    final_prediction = np.array(prediction).flatten()\n",
    "    dataframe = pd.DataFrame({\"Real\":pd.Series(final_target),\n",
    "                              \"Pred\":pd.Series(final_prediction)})\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "backtesting_lstm = backtesting_lstm(raw_data,\n",
    "                 df.dropna(),\n",
    "                 target_variable = 'Spot_electricidad',\n",
    "                 date = \"2021-01-01 00:00:00\",\n",
    "                 lookback = lookback,\n",
    "                 retraining_epochs = 5,\n",
    "                 retraining_steps_per_epoch = 24,\n",
    "                 backtesting_days = test_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:17:14.942166Z",
     "iopub.status.busy": "2022-03-13T17:17:14.941776Z",
     "iopub.status.idle": "2022-03-13T17:17:14.988104Z",
     "shell.execute_reply": "2022-03-13T17:17:14.987450Z",
     "shell.execute_reply.started": "2022-03-13T17:17:14.942132Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b = calculo_metricas(backtesting_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:19:43.688785Z",
     "iopub.status.busy": "2022-03-13T17:19:43.688532Z",
     "iopub.status.idle": "2022-03-13T17:19:43.944307Z",
     "shell.execute_reply": "2022-03-13T17:19:43.943628Z",
     "shell.execute_reply.started": "2022-03-13T17:19:43.688757Z"
    }
   },
   "outputs": [],
   "source": [
    "backtesting_lstm[[\"Real\", \"Pred\"]].plot(figsize = (15,6 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:22:18.831934Z",
     "iopub.status.busy": "2022-03-13T17:22:18.831668Z",
     "iopub.status.idle": "2022-03-13T17:22:19.091386Z",
     "shell.execute_reply": "2022-03-13T17:22:19.090675Z",
     "shell.execute_reply.started": "2022-03-13T17:22:18.831904Z"
    }
   },
   "outputs": [],
   "source": [
    "backtesting_lstm[[\"Real\", \"Pred\"]].loc[24*100:24*150].plot(figsize = (15,6 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Retraining\n",
    "\n",
    "It's going to make bad predictions because the correlations between the variables changeg very much in the periods that we are studying, so the more epochs, the worst metrics for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T17:34:24.414496Z",
     "iopub.status.busy": "2022-03-13T17:34:24.413791Z",
     "iopub.status.idle": "2022-03-13T17:50:48.515936Z",
     "shell.execute_reply": "2022-03-13T17:50:48.515265Z",
     "shell.execute_reply.started": "2022-03-13T17:34:24.414459Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(layers.LSTM(128,\n",
    "                    dropout = 0.1,\n",
    "                    return_sequences = True,\n",
    "                    input_shape = (lookback, raw_data.shape[-1])))\n",
    "\n",
    "model.add(layers.LSTM(64,\n",
    "                      return_sequences = True,\n",
    "                      dropout = 0.1))\n",
    "\n",
    "model.add(layers.LSTM(32,\n",
    "                      return_sequences = True,\n",
    "                      dropout = 0.1))\n",
    "\n",
    "model.add(layers.LSTM(16,\n",
    "                     dropout = 0.1))\n",
    "\n",
    "model.add(layers.Dense(50))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mae', metrics = [rmse_lstm, wmape_lstm])\n",
    "\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_wmape_lstm',\n",
    "                                            patience=50,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    steps_per_epoch = 100,\n",
    "                    epochs = 150,\n",
    "                    validation_data = test_gen,\n",
    "                    validation_steps = test_steps,\n",
    "                    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T18:10:26.662498Z",
     "iopub.status.busy": "2022-03-13T18:10:26.661777Z",
     "iopub.status.idle": "2022-03-13T18:10:58.465557Z",
     "shell.execute_reply": "2022-03-13T18:10:58.464828Z",
     "shell.execute_reply.started": "2022-03-13T18:10:26.662458Z"
    }
   },
   "outputs": [],
   "source": [
    "test_min_index = get_index(df.dropna(), '2021-01-01 00:00:00') - lookback\n",
    "test_max_index = len(df.dropna())\n",
    "test_gen = generator(data = raw_data,\n",
    "                     order = df.columns.tolist(),\n",
    "                     target = 'Spot_electricidad',\n",
    "                     lookback = lookback,\n",
    "                     min_index = test_min_index,\n",
    "                     max_index = test_max_index)\n",
    "\n",
    "def predictions_lstm(raw_data, df, target_variable, date, lookback,\n",
    "                     retraining_epochs, retraining_steps_per_epoch, backtesting_days):\n",
    "    \n",
    "    index_normalization = df.dropna().columns.tolist().index(target_variable)\n",
    "    \n",
    "    generator_samples_test = []\n",
    "    generator_target_test = []\n",
    "\n",
    "    for _ in range(test_steps - 1):\n",
    "        data_sample, target_sample = next(test_gen)\n",
    "        generator_samples_test.append(data_sample)\n",
    "        generator_target_test.append(target_sample)\n",
    "\n",
    "    generator_samples_test = np.array(generator_samples_test)\n",
    "    generator_target_test = np.array(generator_target_test)\n",
    "\n",
    "    # Predictions\n",
    "    preds = []\n",
    "    for rows in generator_samples_test:\n",
    "        preds.append(model.predict(rows))\n",
    "    preds = np.array(preds).flatten()\n",
    "\n",
    "    # Denormalization\n",
    "    target = (generator_target_test * std[index_normalization] + mean[index_normalization]).flatten()\n",
    "    prediction = preds * std[index_normalization] + mean[index_normalization]\n",
    "\n",
    "    # Flat the data and get the dataframe\n",
    "    final_target = np.array(target).flatten()\n",
    "    final_prediction = np.array(prediction).flatten()\n",
    "    dataframe = pd.DataFrame({\"Real\":pd.Series(final_target),\n",
    "                                  \"Pred\":pd.Series(final_prediction)})\n",
    "    return dataframe\n",
    "\n",
    "without_retraining_lstm = predictions_lstm(raw_data,\n",
    "                             df.dropna(),\n",
    "                             target_variable = 'Spot_electricidad',\n",
    "                             date = \"2021-01-01 00:00:00\",\n",
    "                             lookback = lookback,\n",
    "                             retraining_epochs = 5,\n",
    "                             retraining_steps_per_epoch = 24,\n",
    "                             backtesting_days = test_steps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-13T18:10:58.467286Z",
     "iopub.status.busy": "2022-03-13T18:10:58.467039Z",
     "iopub.status.idle": "2022-03-13T18:10:58.729508Z",
     "shell.execute_reply": "2022-03-13T18:10:58.728877Z",
     "shell.execute_reply.started": "2022-03-13T18:10:58.467253Z"
    }
   },
   "outputs": [],
   "source": [
    "without_retraining_lstm[[\"Real\", \"Pred\"]].plot(figsize = (15,6 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
